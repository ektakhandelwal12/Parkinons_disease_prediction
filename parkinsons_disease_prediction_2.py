# -*- coding: utf-8 -*-
"""parkinsons_disease_prediction_2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1axt_rPEQ9n3GSCcPk1OEtfa5KR7-icUd
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.metrics import accuracy_score,mean_squared_error, r2_score

"""Data collection and analysis"""

parkinsons_data=pd.read_csv('/content/parkinsons.csv')

"""MEAN,MEDIAN,MODE"""

# Calculate the mean, mode, and median of the 'MDVP:Fo(Hz)' feature for patients with Parkinson's disease
mean_pd = parkinsons_data[parkinsons_data['status'] == 1]['MDVP:Fo(Hz)'].mean()
mode_pd = parkinsons_data[parkinsons_data['status'] == 1]['MDVP:Fo(Hz)'].mode()[0]
median_pd = parkinsons_data[parkinsons_data['status'] == 1]['MDVP:Fo(Hz)'].median()

# Calculate the mean, mode, and median of the 'MDVP:Fo(Hz)' feature for healthy patients
mean_healthy = parkinsons_data[parkinsons_data['status'] == 0]['MDVP:Fo(Hz)'].mean()
mode_healthy = parkinsons_data[parkinsons_data['status'] == 0]['MDVP:Fo(Hz)'].mode()[0]
median_healthy = parkinsons_data[parkinsons_data['status'] == 0]['MDVP:Fo(Hz)'].median()

print("Mean of 'MDVP:Fo(Hz)' for Parkinson's patients:", mean_pd)
print("Mode of 'MDVP:Fo(Hz)' for Parkinson's patients:", mode_pd)
print("Median of 'MDVP:Fo(Hz)' for Parkinson's patients:", median_pd)

print("Mean of 'MDVP:Fo(Hz)' for healthy patients:", mean_healthy)
print("Mode of 'MDVP:Fo(Hz)' for healthy patients:", mode_healthy)
print("Median of 'MDVP:Fo(Hz)' for healthy patients:", median_healthy)

"""F-test, T-test, and chi-test"""

from scipy.stats import f_oneway, ttest_ind, chi2_contingency

# F-test for the 'MDVP:Fo(Hz)' feature between patients with Parkinson's disease and healthy patients
f_statistic, p_value = f_oneway(parkinsons_data[parkinsons_data['status'] == 1]['MDVP:Fo(Hz)'], parkinsons_data[parkinsons_data['status'] == 0]['MDVP:Fo(Hz)'])
print("F-statistic for 'MDVP:Fo(Hz)':", f_statistic)
print("p-value for 'MDVP:Fo(Hz)':", p_value)

# T-test for the 'MDVP:Fo(Hz)' feature between patients with Parkinson's disease and healthy patients
t_statistic, p_value = ttest_ind(parkinsons_data[parkinsons_data['status'] == 1]['MDVP:Fo(Hz)'], parkinsons_data[parkinsons_data['status'] == 0]['MDVP:Fo(Hz)'])
print("T-statistic for 'MDVP:Fo(Hz)':", t_statistic)
print("p-value for 'MDVP:Fo(Hz)':", p_value)

# Chi-test for the contingency table of 'gender' and 'status' columns
contingency_table = pd.crosstab(parkinsons_data['DFA'], parkinsons_data['status'])
chi_statistic, p_value, dof, expected = chi2_contingency(contingency_table)
print("Chi-square statistic for 'gender' and 'status':", chi_statistic)
print("p-value for 'gender' and 'status':", p_value)

"""LINEAR REGRESSION"""

train_data = parkinsons_data[:150]
test_data = parkinsons_data[150:]

X_train = train_data.drop(['name', 'status'], axis=1)
y_train = train_data['status']
X_test = test_data.drop(['name', 'status'], axis=1)
y_test = test_data['status']

regressor = LinearRegression()

regressor.fit(X_train, y_train)

y_pred = regressor.predict(X_test)

print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))
print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))

"""printing the first 5 rows from the database"""

parkinsons_data.head()

"""number of rows and column in the dataset"""

parkinsons_data.shape

parkinsons_data.info()

"""Getting some statistical measure"""

parkinsons_data.describe()

"""distribution of Target variable"""

parkinsons_data['status'].value_counts()

"""grouping the data based on the target value"""

parkinsons_data.groupby('status').mean()

"""Data Preprocessing

Seperaing the Feature and Target
"""

X=parkinsons_data.drop(columns=['name','status'],axis=1)
Y=parkinsons_data['status']

print(X)

print(Y)

"""Logistic Reggression"""

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=2)

lr_model = LogisticRegression()

lr_model.fit(X_train, Y_train)

y_pred = lr_model.predict(X_test)

accuracy = accuracy_score(Y_test, y_pred)
print("Accuracy of the model:", accuracy)

"""RANDOM FOREST-CROSS VAIDATION"""

rfc_model = RandomForestClassifier()

scores = cross_val_score(rfc_model, X_train, Y_train, cv=10)

print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

rfc_model.fit(X_train, Y_train)

y_pred = rfc_model.predict(X_test)

accuracy = accuracy_score(Y_test, y_pred)
print("Accuracy of the model:", accuracy)

"""KNN """

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, Y_train)

# Make predictions on the testing data and calculate accuracy
y_pred = knn.predict(X_test)
accuracy = accuracy_score(Y_test, y_pred)

print("Accuracy:", accuracy)

"""SVM"""

from sklearn.svm import SVC

svm = SVC(kernel='linear', C=1.0, random_state=42)
svm.fit(X_train, Y_train)

# Make predictions on the testing data and calculate accuracy
y_pred = svm.predict(X_test)
accuracy = accuracy_score(Y_test, y_pred)

print("Accuracy:", accuracy)

print(X.shape,X_train.shape,X_test.shape)

"""Data Standarization"""

scaler=StandardScaler()

"""K-MEANS"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

X_std=scaler.fit_transform(X)

X_train=scaler.transform(X_train)

X_test=scaler.transform(X_test)

# Determine the optimal number of clusters using silhouette score
silhouette_scores = []
for n_clusters in range(2, 11):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_std)
    silhouette_scores.append(silhouette_score(X_std, cluster_labels))

optimal_n_clusters = silhouette_scores.index(max(silhouette_scores)) + 2

# Create the KMeans classifier with optimal number of clusters and fit it to the standardized data
kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42)
kmeans.fit(X_std)

# Add the predicted cluster labels to the original DataFrame
parkinsons_data['cluster'] = kmeans.labels_

# Calculate the percentage of patients with Parkinson's disease in each cluster
cluster_counts = parkinsons_data.groupby('cluster')['status'].value_counts(normalize=True)

print(cluster_counts)

"""PCA"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Create the PCA transformer and fit it to the standardized data
pca = PCA(n_components=2, random_state=42)
pca.fit(X_std)

# Transform the standardized data into principal components
X_pca = pca.transform(X_std)

# Plot the principal components
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=parkinsons_data['status'], cmap='coolwarm')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

input_data=(95.73000,132.06800,91.75400,0.00551,0.00006,0.00293,0.00332,0.00880,0.02093,0.19100,0.01073,0.01277,0.01717,0.03218,0.01070,21.81200,0.615551,0.773587,-5.498678,0.327769,2.322511,0.231571)

#change the input data to a numpy array
input_data_as_numpy_array=np.asarray(input_data)

#reshape the numpy array
input_data_reshaped=input_data_as_numpy_array.reshape(1,-1)

#standardize the data
std_data=scaler.transform(input_data_reshaped)

model=svm.SVC(kernel='linear')

prediction=model.predict(std_data)

model=svm.SVC(kernel='linear')

prediction=model.predict(std_data)
print(prediction)